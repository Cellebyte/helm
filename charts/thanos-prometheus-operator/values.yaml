# Default values for thanos-prometheus-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


# Enables ingress support for the prometheus instances

federation:
  enabled: true
  ingress:
    baseFQDN: k8s.cluster.local
    annotations:
      nginx.ingress.kubernetes.io/auth-realm: Authentication Required
      nginx.ingress.kubernetes.io/auth-secret: basic-auth
      nginx.ingress.kubernetes.io/auth-type: basic
    servicePort: web
    tls: no

dashboards:
  enabled: true

operator:
  enabled: true
  rbac:
    create: true
    apiVersion: v1beta1
    pspEnabled: true
  operator:
    enabled: true
    image:
      registry: quay.io
      repository: coreos/prometheus-operator
      tag: v0.37.0
      pullPolicy: IfNotPresent
    serviceAccount:
      create: true
    securityContext:
      enabled: true
      runAsUser: 1001
      fsGroup: 1001
    service:
      type: ClusterIP
      port: 8080
      annotations: {}
    createCustomResource: true
    ## Prometheus Operator CRD deletion policy
    ## ref: https://v3.helm.sh/docs/topics/charts_hooks/
    ##
    # customResourceDeletePolicy: before-hook-creation
    serviceMonitor:
      enabled: true
      interval: 15s

    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

    livenessProbe:
      initialDelaySeconds: 120
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1

    readinessProbe:
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 6
      successThreshold: 1

    logLevel: info
    logFormat: logfmt

    ## If true, the operator will create and maintain a service for scraping kubelets
    ##
    kubeletService:
      enabled: true
      namespace: kube-system

    ## Configmap-reload image to use for reloading configmaps
    ##
    configmapReload:
      image:
        registry: quay.io
        repository: coreos/configmap-reload
        tag: v0.0.1
    prometheusConfigReloader:
      image:
        registry: quay.io
        repository: coreos/prometheus-config-reloader
        tag: v0.37.0
  prometheus:
    enabled: true

    image:
      registry: quay.io
      repository: prometheus/prometheus
      tag: v2.16.0

    serviceAccount:
      create: true

    securityContext:
      enabled: true
      runAsUser: 1001
      fsGroup: 1001

    podDisruptionBudget:
      enabled: true
      minAvailable: 1

    service:
      type: ClusterIP
      port: 9090

    serviceMonitor:
      enabled: true
      interval: 15s

    resources:
      requests:
        memory: 400Mi

    listenLocal: false

    externalLabels:
      customer: "<customer>"
      cluster: "<cluster>"
    
    additionalScrapeConfigsExternal: false
    ## ConfigMaps that should be mounted into the Prometheus Pods
    ##
    configMaps: []

    nodeSelector:
      kubernetes.io/os: linux
    ## The query command line flags when starting Prometheus
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec
    ##
    querySpec: {}

    ## Namespaces to be selected for PrometheusRules discovery
    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
    ##
    ruleNamespaceSelector: {}

    ## PrometheusRules to be selected for target discovery.
    ## If {}, select all ServiceMonitors
    ##
    ruleSelector: {}

    ## ServiceMonitors to be selected for target discovery.
    ## If {}, select all ServiceMonitors
    ##
    serviceMonitorSelector: {}
      # matchLabels:
      #   foo: bar

    ## Namespaces to be selected for ServiceMonitor discovery.
    ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
    ##
    serviceMonitorNamespaceSelector: {}

    ## How long to retain metrics
    ##
    retention: 20d

    ## Maximum size of metrics
    ##
    ## retentionSize: ""

    ## Enable compression of the write-ahead log using Snappy.
    ##
    walCompression: false

    ## Desired number of Prometheus nodes
    ##
    replicaCount: 2

    ## Log level for Prometheus
    ##
    logLevel: info

    ## Log format for Prometheus
    ##
    logFormat: logfmt

    ## Standard objectâ€™s metadata
    ## ref: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
    ##
    podMetadata: {}
    # labels:
    #   app: prometheus
    #   k8s-app: prometheus

    ## The remote_read spec configuration for Prometheus.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
    ##
    remoteRead: []
    # - url: http://remote1/read

    ## The remote_write spec configuration for Prometheus.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
    ##
    remoteWrite: []
    # - url: http://remote1/push

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    storageSpec: 
      emptyDir:
        medium: ""
      # volumeClaimTemplate:
      #   apiVersion: v1
      #   kind: PersistentVolumeClaim
      #   spec:
      #     accessModes:
      #     - ReadWriteOnce
      #     resources:
      #       requests:
      #         storage: 50Gi
      #     storageClassName: "<storage_class_name>"

    ## Thanos sidecar container configuration
    ##
    thanos:
      create: true
      image:
        registry: quay.io
        repository: thanos/thanos
        tag: v0.10.1
        pullPolicy: IfNotPresent
      service:
        type: ClusterIP
        port: 10901

  alertmanager:
    enabled: true
    image:
      registry: quay.io
      repository: prometheus/alertmanager
      tag: v0.20.0

    serviceAccount:
      create: true

    securityContext:
      enabled: true
      runAsUser: 1001
      fsGroup: 1001

    podDisruptionBudget:
      enabled: true
      minAvailable: 1

    service:
      type: ClusterIP
      port: 9093

    serviceMonitor:
      enabled: true
      interval: 15s

    ingress:
      enabled: false
      certManager: false
      annotations: {}
      hosts:
      - name: alertmanager.k8s.cluster.local
        path: /

      ## The tls configuration for the ingress
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## tls:
      ## - hosts:
      ##     - alertmanager.local
      ##   secretName: alertmanager.local-tls



    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: ['job']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
      receivers:
      - name: 'null'

    replicaCount: 3

    logLevel: info

    logFormat: logfmt

    retention: 120h

    ## Alertmanager StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    storageSpec:
      emptyDir:
        medium: ""
      # volumeClaimTemplate:
        ## after https://github.com/coreos/prometheus-operator/pull/3049 gets introduced
        ## in a new version these two lines can be readded
        # apiVersion: v1
        # kind: PersistentVolumeClaim
        # spec:
        #   accessModes:
        #   - ReadWriteOnce
        #   resources:
        #     requests:
        #       storage: 2Gi
        #   storageClassName: "<storage_class_name>"

    listenLocal: false

  ## Exporters
  exporters:
    node-exporter:
      enabled: true

    kube-state-metrics:
      enabled: true

    kubeControllerManager:
      enabled: false
      service:
        port: 10252
        targetPort: 10252
        # selector:
        #   component: kube-controller-manager
      serviceMonitor:
        interval: 15s
        https: true
        insecureSkipVerify: null
        serverName: null
        metricRelabelings: []
        relabelings: []

    kubeScheduler:
      enabled: true
      service:
        port: 10251
        targetPort: 10251
        # selector:
        #   component: kube-scheduler

      serviceMonitor:
        interval: 15s
        https: false
        insecureSkipVerify: null
        serverName: null
        metricRelabelings: []
        relabelings: []

    kubeProxy:
      enabled: false
      service:
        port: 10249
        targetPort: 10249
        # selector:
        #   k8s-app: kube-proxy

      serviceMonitor:
        interval: 15s
        https: true
        metricRelabelings: []
        relabelings: []

    coredns:
      enabled: true
      service:
        port: 9153
        targetPort: 9153
        #   selector:
        #   k8s-app: kube-dns
      serviceMonitor:
        interval: 15s
        metricRelabelings: []
        relabelings: []

    etcd:
      enabled: false
      service:
        port: 2379
        targetPort: 2379
        # selector:
        #   component: etcd

      serviceMonitor:
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
        scheme: http
        insecureSkipVerify: false
        serverName: ""
        caFile: ""
        certFile: ""
        keyFile: ""
        metricRelabelings: []
        relabelings: []


  ## Node Exporter deployment configuration
  node-exporter:
    image:
      registry: quay.io
      repository: prometheus/node-exporter
      tag: v0.18.1

    service:
      labels:
        jobLabel: node-exporter

    serviceMonitor:
      enabled: true
      jobLabel: jobLabel

    extraArgs:
      collector.filesystem.ignored-mount-points: "^/(dev|proc|sys|var/lib/docker/.+)($|/)"
      collector.filesystem.ignored-fs-types: "^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$"

  kube-state-metrics:
    image:
      registry: quay.io
      repository: coreos/kube-state-metrics
      tag: v1.9.5
    replicaCount: 2
    serviceMonitor:
      enabled: true
    hostNetwork: false

  ## Component scraping the kube-apiserver
  kubeApiServer:
    ## Create a ServiceMonitor to scrape kube-apiserver service
    enabled: true
    serviceMonitor:
      interval: 15s

  # Component scraping for kubelet and kubelet hosted cAdvisor
  kubelet:
    ## Create a ServiceMonitor to scrape kubelet service
    enabled: true

    ## Namespace where kubelet service is deployed
    namespace: kube-system

    serviceMonitor:
      https: true
      interval: 15s

query:
  enabled: true
  image:
    registry: quay.io
    repository: thanos/thanos
    tag: v0.10.1
    pullPolicy: IfNotPresent
  clusterDomain: cluster.local
  querier:
    enabled: true
    logLevel: info
    replicaLabel: prometheus_replica
    dnsDiscovery:
      enabled: true
      ## Sidecars service name to discover them using DNS discovery
      ##
      sidecarsService: thanos-prometheus-operator-prometheus-thanos
      ## Sidecars namespace to discover them using DNS discovery
      ##
      sidecarsNamespace: monitoring-system

    ## Querier Service Discovery Configuration
    ## Specify content for servicediscovery.yml
    ##
    # sdConfig:

    ## ConfigMap with Querier Service Discovery Configuration
    ## NOTE: This will override querier.sdConfig
    ##
    # existingSDConfigmap:

    ## Extra Flags to passed to Thanos Querier
    ##
    extraFlags: {}

    ## Number of Thanos Querier replicas to deploy
    ##
    replicaCount: 2

    ## StrategyType, can be set to RollingUpdate or Recreate by default.
    ##
    strategyType: RollingUpdate

    ## Affinity for pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ##
    affinity: {}

    ## Node labels for pod assignment. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Tolerations for pod assignment. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: {}

    ## Pod priority
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    # priorityClassName: ""

    ## K8s Security Context for Thanos Querier pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    securityContext:
      enabled: true
      fsGroup: 1001
      runAsUser: 1001
    
    ## Thanos Querier containers' resource requests and limits
    ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      limits: {}
      #   cpu: 100m
      #   memory: 128Mi
      requests: {}
      #   cpu: 100m
      #   memory: 128Mi
    
    ## Thanos Querier pods' liveness and readiness probes. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ##
    livenessProbe:
      httpGet:
        path: /-/healthy
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 30
      # periodSeconds: 10
      # successThreshold: 1
      # failureThreshold: 6
    readinessProbe:
      httpGet:
        path: /-/ready
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 30
      # periodSeconds: 10
      # successThreshold: 1
      # failureThreshold: 6

    ## Service paramaters
    ##
    service:
      type: ClusterIP
      http:
        port: 9090
      grpc:
        port: 10901

    ## Autoscaling parameters
    ##
    autoscaling:
      enabled: false
      #  minReplicas: 1
      #  maxReplicas: 11
      #  targetCPU: 50
      #  targetMemory: 50

    ## Querier Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      create: true
      minAvailable: 1

  bucketweb:
    enabled: false

  compactor:
    enabled: false

  storegateway:
    enabled: false

  ruler:
    enabled: false

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      
      interval: 15s

grafana:
  enabled: true
  image:
    registry: docker.io
    repository: grafana/grafana
    tag: 6.6.2
    pullPolicy: IfNotPresent

  replicaCount: 1

  updateStrategy:
    type: RollingUpdate

  admin:
    user: "admin"
    password: "admin"

  ## Grafana plugins that will be installed
  ## Specify plugins as a list separated by commas ( you will need to scape them when specifying from command line )
  ## Example:
  ## plugins: grafana-kubernetes-app,grafana-example-app
  ##
  # plugins:

  ## Ldap configuration for Grafana
  ##
  ldap:
    enabled: false
    allowSignUp: false
    ## configMap with LDAP configuration file (ldap.toml)
    # configMapName:

  ## An array to add extra env vars
  ## For example:
  ## extraEnvVars:
  ##  - name: GF_DEFAULT_INSTANCE_NAME
  ##    value: my-instance
  ##
  extraEnvVars: {}

  ## An array to add extra configmaps:
  ## For example:
  ## extraConfigmaps:
  ##   - name: myconfigmap
  ##     mountPath: /opt/bitnami/desired-path
  ##     subPath: file-name.extension (optional)
  ##     readOnly: true
  ##
  extraConfigmaps: {

  }

  ## Parameters to override the default grafana.ini and custom.ini files.
  ## It is needed to create a configmap or a secret containing the grafana.ini and custom.ini files.
  ##
  config:
    useGrafanaIniFile: true
    grafanaIniSecret: grafana-config
    useCustomIniFile: false
    customIniConfigMap:
    customIniSecret:

  ## Create dasboard provider to load dashboards, a default one is created to load
  ## dashboards from "/opt/bitnami/grafana/dashboards"
  ##
  dashboardsProvider:
    enabled: true
    ## ConfigMap with a custom provider file.
    ## Important to set the Path to "/opt/bitnami/grafana/dashboards"
    # configMapName:

  ## Create dashboards from a custom configMap that contains the file.
  ## They will be mounted by the default dashboard provider if it is enabled
  ## one file per configmap.
  ## Use an array with the configMap names.
  dashboardsConfigMaps:
  - configMapName: grafana-dashboard-etcd
    fileName: etcd.json
  - configMapName: grafana-dashboard-apiserver
    fileName: apiserver.json
  - configMapName: grafana-dashboard-cluster-total
    fileName: cluster-total.json
  - configMapName: grafana-dashboard-controller-manager
    fileName: controller-manager.json
  - configMapName: grafana-dashboard-k8s-resources-cluster
    fileName: k8s-resources-cluster.json
  - configMapName: grafana-dashboard-k8s-resources-namespace
    fileName: k8s-resources-namespace.json
  - configMapName: grafana-dashboard-k8s-resources-node
    fileName: k8s-resources-node.json
  - configMapName: grafana-dashboard-k8s-resources-pod
    fileName: k8s-resources-pod.json
  - configMapName: grafana-dashboard-k8s-resources-workload
    fileName: k8s-resources-workload.json
  - configMapName: grafana-dashboard-k8s-resources-workloads-namespace
    fileName: k8s-resources-workloads-namespace.json
  - configMapName: grafana-dashboard-kubelet
    fileName: kubelet.json
  - configMapName: grafana-dashboard-namespace-by-pod
    fileName: namespace-by-pod.json
  - configMapName: grafana-dashboard-namespace-by-workload
    fileName: namespace-by-workload.json
  - configMapName: grafana-dashboard-node-cluster-rsrc-use
    fileName: node-cluster-rsrc-use.json
  - configMapName: grafana-dashboard-node-rsrc-use
    fileName: node-rsrc-use.json
  - configMapName: grafana-dashboard-nodes
    fileName: nodes.json
  - configMapName: grafana-dashboard-persistentvolumesusage
    fileName: persistentvolumesusage.json
  - configMapName: grafana-dashboard-pod-total
    fileName: pod-total.json
  - configMapName: grafana-dashboard-pods
    fileName: pods.json
  - configMapName: grafana-dashboard-prometheus-remote-write
    fileName: prometheus-remote-write.json
  - configMapName: grafana-dashboard-prometheus
    fileName: prometheus.json
  - configMapName: grafana-dashboard-proxy
    fileName: proxy.json
  - configMapName: grafana-dashboard-scheduler
    fileName: scheduler.json
  - configMapName: grafana-dashboard-statefulset
    fileName: statefulset.json
  - configMapName: grafana-dashboard-workload-total
    fileName: workload-total.json
  - configMapName: grafana-dashboard-coredns
    fileName: coredns.json
  ## Create datasources from a custom secret
  ## The secret must contain the files
  ##
  datasources:
    secretName: grafana-datasources

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: false
    ## wordpress data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    ##
    ## If you want to reuse an existing claim, you can pass the name of the PVC using
    ## the existingClaim variable
    # existingClaim: your-claim
    accessMode: ReadWriteOnce
    size: 10Gi

  ## Grafana containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 120
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  ## Service parameters
  ##
  service:
    type: ClusterIP
    port: 3000
    annotations: {}

  ## Configure the ingress resource that allows you to access the
  ## Grafana web. Set up the URL
  ## ref: http://kubernetes.io/docs/user-guide/ingress/
  ##
  ingress:
    ## Set to true to enable ingress record generation
    enabled: true

    ## Set this to true in order to add the corresponding annotations for cert-manager
    certManager: false

    ## Ingress annotations done as key:value pairs
    ## For a full list of possible ingress annotations, please see
    ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
    ##
    ## If tls is set to true, annotation ingress.kubernetes.io/secure-backends: "true" will automatically be set
    ## If certManager is set to true, annotation kubernetes.io/tls-acme: "true" will automatically be set
    # annotations:
    #   kubernetes.io/ingress.class: nginx

    ## The list of hostnames to be covered with this ingress record.
    ## Most likely this will be just one host, but in the event more hosts are needed, this is an array
    hosts:
      - name: grafana.k8s.cluster.local
        paths: ["/"]
        ## Set this to true in order to enable TLS on the ingress record
        tls: false
        tlsHosts:
        - grafana.k8s.cluster.local
        tlsSecret: grafana.local-tls

  ## SecurityContext configuration
  ##
  securityContext:
    enabled: true
    runAsUser: 1001
    fsGroup: 1001
    runAsNonRoot: true

  ## Grafana containers' resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 250m
      memory: 256Mi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}

  ## Pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}

  ## Prometheus metrics
  ##
  metrics:
    enabled: true

    ## Prometheus Operator ServiceMonitor configuration
    ##
    service:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/metrics"

    serviceMonitor:
      enabled: true
      ## Namespace in which Prometheus is running
      ##
      

      ## Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: 15s

      ## Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: 15s


