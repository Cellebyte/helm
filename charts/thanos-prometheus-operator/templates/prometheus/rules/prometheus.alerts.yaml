{{- if .Values.operator.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: "{{ include "thanos-prometheus-operator.name" . }}-prometheus"
    role: default
  name: prometheus.alerts
spec:
  groups:
  - name: prometheus.alerts
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: 'max_over_time(prometheus_config_last_reload_successful{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m])== 0'
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: 'Alert notification queue of Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is running full.'
        summary: Prometheus alert notification queue predicted to run full in less than 30m.
      expr: (predict_linear(prometheus_notifications_queue_length{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m], 60 * 30)>  min_over_time(prometheus_notifications_queue_capacity{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]))
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{`{{ printf "%.1f" $value }}`}}% errors while sending alerts from Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to Alertmanager {{`{{$labels.alertmanager}}`}}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      expr: '(rate(prometheus_notifications_errors_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m])/  rate(prometheus_notifications_sent_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]))* 100> 1'
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
      annotations:
        description: '{{`{{ printf "%.1f" $value }}`}}% minimum errors while sending alerts from Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} to any Alertmanager.'
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      expr: 'min without(alertmanager) (rate(prometheus_notifications_errors_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) / rate(prometheus_notifications_sent_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]))* 100> 3'
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not connected to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: 'max_over_time(prometheus_notifications_alertmanagers_discovered{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) < 1'
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected {{`{{$value| humanize}}`}} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: 'increase(prometheus_tsdb_reloads_failures_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[3h]) > 0'
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has detected {{`{{$value| humanize}}`}} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: 'increase(prometheus_tsdb_compactions_failed_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[3h]) > 0'
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is not ingesting samples.
        summary: Prometheus is not ingesting samples.
      expr: 'rate(prometheus_tsdb_head_samples_appended_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) <= 0'
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping {{`{{ printf "%.4g" $value  }}`}} samples/s with different values but duplicated timestamp.
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: 'rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) > 0'
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} is dropping {{`{{printf "%.4g" $value  }}`}} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: 'rate(prometheus_target_scrapes_sample_out_of_order_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) > 0'
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusRemoteStorageFailures
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} failed to send {{`{{ printf "%.1f" $value }}`}}% of the samples to {{`{{ if $labels.queue }}{{ $labels.queue }}{{ else }}{{ $labels.url }}{{ end }}`}}.
        summary: Prometheus fails to send samples to remote storage.
      expr: '(rate(prometheus_remote_storage_failed_samples_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) (rate(prometheus_remote_storage_failed_samples_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) + rate(prometheus_remote_storage_succeeded_samples_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]))* 100> 1'
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteBehind
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} remote write is {{`{{ printf "%.1f" $value }}`}}s behind for {{`{{ if $labels.queue }}{{ $labels.queue }}{{ else }}{{ $labels.url }}{{ end }}`}}.
        summary: Prometheus remote write is behind.
      expr: '(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}}[5m])- on(job, instance) group_right max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) 120'
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: 'Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} remote write desired shards calculation wants to run {{`{{ $value }}`}} shards, which is more than the max of \{\{ printf `prometheus_remote_storage_shards_max{instance="%s",job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}` {{` $labels.instance `}} | query | first | value \}\}`}}.'
        summary: Prometheus remote write desired shards calculation wants to run morethan configured max shards.
      expr: '(max_over_time(prometheus_remote_storage_shards_desired{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) max_over_time(prometheus_remote_storage_shards_max{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]))"'
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has failed to evaluate {{`{{ printf "%.0f" $value }}`}} rules in the last 5m.
        summary: Prometheus is failing rule evaluations.
      expr: 'increase(prometheus_rule_evaluation_failures_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) > 0'
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has missed {{`{{ printf "%.0f" $value }}`}} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: 'increase(prometheus_rule_group_iterations_missed_total{job="{{ include "thanos-prometheus-operator.name" . }}-prometheus",namespace="{{ .Release.Namespace }}"}[5m]) > 0'
      for: 15m
      labels:
        severity: warning
{{- end }}
